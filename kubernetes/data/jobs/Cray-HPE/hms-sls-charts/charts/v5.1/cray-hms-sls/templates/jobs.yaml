---
apiVersion: batch/v1
kind: Job
metadata:
  name: cray-sls-init-load-nameserver-getter
  annotations:
    "helm.sh/hook": pre-install
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "-5"
spec:
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: “false”
    spec:
      restartPolicy: OnFailure
      serviceAccountName: "cray-hms-sls"
      containers:
        - name: nameserver-getter
          image: "{{ .Values.kubectl.image.repository }}:{{ .Values.kubectl.image.tag }}"
          imagePullPolicy: "{{ .Values.kubectl.image.pullPolicy }}"
          command:
          - /bin/sh
          - -c
          - set -exu;
            sed -n '/nameserver/p' /host/resolv.conf > /tmp/nameserver;
            echo "The following nameserver(s) was found from the hosts resolve.conf";
            cat /tmp/nameserver;
            kubectl -n services create configmap cray-sls-init-loader-nameserver --from-file=/tmp/nameserver
            --dry-run -o yaml | kubectl apply -f -
          volumeMounts:
          - mountPath: /host/resolv.conf
            name: resolv-dir
            subPath: resolv.conf
            readOnly: true
      volumes:
      # On the NCNs the file /etc/resolv.conf is a a symlink to /run/netconfig/resolv.conf
      # This is true on both vshasta & baremetal, and k8s hostPath did not work with the symlink
      - hostPath:
          path: {{ .Values.namespaceGetter.resolvDir | quote }}
        name: resolv-dir

---
apiVersion: batch/v1
kind: Job
metadata:
  name: cray-sls-init-load-deleter
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,pre-rollback
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "0"
spec:
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: “false”
    spec:
      restartPolicy: OnFailure
      serviceAccountName: "cray-hms-sls"
      containers:
        - name: job-deleter
          image: "{{ .Values.kubectl.image.repository }}:{{ .Values.kubectl.image.tag }}"
          imagePullPolicy: "{{ .Values.kubectl.image.pullPolicy }}"
          command:
          - /bin/sh
          - -c
          - set -x;
            echo "Deleting Jobs";
            kubectl -n services delete job cray-sls-init-load;
            while [ "`kubectl -n services get pods -l app=cray-sls-init-load -o jsonpath='{.items}'`" != "[]" ];
            do
              echo "Waiting for previous job to be removed entirely...";
              sleep 1;
            done;
            echo "Old job deleted.";
            exit 0
---
apiVersion: batch/v1
kind:       Job
metadata:
  name: cray-sls-init-load
  labels:
    app: cray-sls-init-load
spec:
  ttlSecondsAfterFinished: {{ .Values.slsInitJobTTL }}
  template:
    metadata:
      labels:
        app: cray-sls-init-load
    spec:
      restartPolicy:      OnFailure
      securityContext:
        fsGroup: 65534
        runAsUser: 65534
      serviceAccountName: "jobs-watcher"
      initContainers:
        - name: cray-sls-init
          image: "{{ .Values.image.repository }}:{{ include "cray-sls.imageTag" . }}"
          imagePullPolicy: "{{ .Values.image.pullPolicy }}"
          command: ["/entrypoint.sh"]
          args: ["sls-init"]
          env:
            - name: POSTGRES_HOST
              value: "cray-sls-postgres"
            - name: DBUSER
              valueFrom:
                secretKeyRef:
                  name: "slsuser.cray-sls-postgres.credentials"
                  key:  "username"
            - name: DBPASS
              valueFrom:
                secretKeyRef:
                  name: "slsuser.cray-sls-postgres.credentials"
                  key:  "password"
            - name: SCHEMA_VERSION
              value: "{{ .Values.schemaVersion }}"
          volumeMounts:
            - mountPath: "/persistent_migrations"
              name: schema
      containers:
        - name: cray-sls-loader
          env:
            - name: SLS_FILE_PATH
              value: "/sls/sls_input_file.json"
            - name: SLS_URL
              value: "http://cray-sls"
            - name: SLS_LOADER_FORCE_UPLOAD
              value: "{{ .Values.loader.forceUpload }}"
            - name: SLS_LOADER_CHECK_S3_MARKER
              value: "{{ .Values.loader.checkS3Marker }}"
            - name: SLS_LOADER_CHECK_SLS_CONTENTS
              value: "{{ .Values.loader.checkSLSContents }}"
            - name: S3_ENDPOINT
              valueFrom:
                secretKeyRef:
                  name: sls-s3-credentials
                  key: s3_endpoint
            - name: S3_BUCKET
              value: sls
            - name: S3_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: sls-s3-credentials
                  key: access_key
            - name: S3_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: sls-s3-credentials
                  key: secret_key
            - name: MAX_PING_BUCKET_ATTEMPTS
              value: "{{ .Values.loader.max_ping_bucket_attempts }}"
            - name: USE_S3_DNS_HACK
              value: "{{ .Values.loader.use_s3_dns_hack }}"
            - name: S3_DNS_LOOKUP_ATTEMPTS
              value: "{{ .Values.loader.s3_dns_lookup_attempts }}"
            - name: PIT_NAMESERVER
              valueFrom:
                configMapKeyRef:
                  name: cray-sls-init-loader-nameserver
                  key: nameserver
          image: "{{ .Values.image.repository }}:{{ include "cray-sls.imageTag" . }}"
          imagePullPolicy: "{{ .Values.image.pullPolicy }}"
          command: ["/entrypoint.sh"]
          args: ["sls-loader"]
      volumes:
        - name: schema
          persistentVolumeClaim:
            claimName: cray-hms-sls-schema-pvc
